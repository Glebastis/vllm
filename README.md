## Запуск vLLM в докере

### Пререквизиты:

1. На машине есть видеокарта с поддержкой CUDA (добавить версию) и установлены драйвера
2. Модель скачана на машину (Не обязательно, в случае скачивания модели из HF)


### Сборка и запуск контейнера с vLLM

1. Скопируйте файл dev-env в .env и разместите его в том же каталоге что и docker-compose.yml
2. Заполните переменные в .env файле

  - `VLLM_PATH_TO_MODEL` - Абсолютный путь до модели на диске или название модели из HF
  - `VLLM_PORT` - Порт для vLLM (доступный из хостовой машины) 
  - `VLLM_API_KEY` -  Токен для доступа к vLLM
  - `VLLM_SERVED_MODEL_NAME` - Псевдоним для названия модели
  - `VLLM_GPU_MEMORY_UTILIZATION` - Доля допустимого использования памяти GPU для vLLM по умолчанию 0.5>
  - `VLLM_TENSOR_PARALLEL_SIZE`  - Установите значение, равное количеству видеокарт, подробнее см [документацию vLLM.](https://docs.vllm.ai/en/latest/serving/distributed_serving.html)

Не изменяйте следующие значения без необходимости:

  - `VLLM_INTERNAL_PORT`=3423 - Порт для VLLM внутри контейнера 
  - `VLLM_INTERNAL_HOST`="0.0.0.0" - IP для VLLM внутри контейнера 
  - `HF_HOME`="/cache/huggingface" - Путь до каталога с кэшем HF внутри контейнера
  - `HF_HOME_HOST` - Укажите путь до локального кэша HF если вы хотите, чтобы vLLM скачала модель из  HF.

Если вы хотите чтобы vLLM скачала модель из HF, то:
  - укажите путь до каталога с кэшем Hugginface в .env файле в переменной HF_LOCAL_HOME (по умолчанию "$HOME/.cache/huggingface/hub", или проверьте в переменной окружения HF_HOME)
  - вы можете указать любой каталог для передачи в контейнер в качестве каталога для кэша HF, не обязательно ваш существующий кэш.

3. Запустите сборку контейнера 

  - Для использования самостоятельно скачанных моделей 

```sh
docker compose  --profile local up -d --build 
```

  - Для использования скачивания моделей из huggingface hub \

```sh
docker compose  --profile hf up -d --build 
```