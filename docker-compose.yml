version: "3.7"

services:
  vllm-local: &default
    build:
      dockerfile: ./Dockerfile.vllm
    restart: unless-stopped
    hostname: vllm
    ipc: host
    runtime: nvidia
    env_file:
      - .env
    environment:
      TZ: "Europe/Moscow"
    ports:
      - ${VLLM_PORT:-3423}:$VLLM_INTERNAL_PORT
    volumes:
      - $VLLM_PATH_TO_MODEL:$VLLM_PATH_TO_MODEL
    deploy:
      resources:
        reservations:
         devices:
           - driver: nvidia
             count: all # alternatively, use `count: all` for all GPUs (default 1)
             capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${VLLM_INTERNAL_PORT}/metrics"]
      timeout: 60s
      retries: 5
    entrypoint: [
      "python3", "-m", "vllm.entrypoints.openai.api_server",
      "--host", $VLLM_INTERNAL_HOST, "--port", $VLLM_INTERNAL_PORT,
      "--api-key", $VLLM_API_KEY,
      "--model", $VLLM_PATH_TO_MODEL,
      "--served-model-name", $VLLM_SERVED_MODEL_NAME,
      "--gpu-memory-utilization", "${VLLM_GPU_MEMORY_UTILIZATION:-0.9}",
      "--tensor-parallel-size", $VLLM_TENSOR_PARALLEL_SIZE,
      "--disable-custom-all-reduce",
      "--uvicorn-log-level", "debug",
      "--download-dir", $HF_HOME,
      # "--ssl-keyfile", "/secret/privkey.pem",
      # "--ssl-certfile", "/secret/fullchain.pem"
      ]
    profiles:
      - local

  vllm-hf: 
    <<: *default
    volumes:
      - $HF_HOME_HOST:$HF_HOME
    profiles:
      - hf