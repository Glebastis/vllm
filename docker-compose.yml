version: "3.7"

services:
  vllm-local: &default
    build:
      context: .
      dockerfile: Dockerfile.vllm
    image: vllm
    container_name: vllm
    restart: unless-stopped
    hostname: vllm
    ipc: host
    runtime: nvidia
    env_file:
      - .env
    environment:
      TZ: "Europe/Moscow"
    ports:
      - ${VLLM_PORT:-3423}:$VLLM_INTERNAL_PORT
    volumes:
      - $PATH_TO_MODELS:/models
    deploy:
      resources:
        reservations:
         devices:
           - driver: nvidia
             count: 1 # alternatively, use `count: all` for all GPUs (default 1)
             capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${VLLM_INTERNAL_PORT}/metrics"]
      timeout: 60s
      retries: 5
    entrypoint: [
      "python3", "-m", "vllm.entrypoints.openai.api_server",
      "--host", $VLLM_INTERNAL_HOST, "--port", $VLLM_INTERNAL_PORT,
      "--api-key", $VLLM_API_KEY,
      "--model", /models/$MODEL_NAME, #/$MODEL_FILE,
      "--served-model-name", $VLLM_SERVED_MODEL_NAME,
      "--gpu-memory-utilization", "${VLLM_GPU_MEMORY_UTILIZATION:-0.9}",
      "--tensor-parallel-size", $VLLM_TENSOR_PARALLEL_SIZE,
      "--disable-custom-all-reduce",
      "--uvicorn-log-level", "debug",
      "--download-dir", $HF_HOME,
      "--max_num_seqs", "32",
      "--max_model_len", "32768",
      # "--hf-config-path", "/models/$MODEL_NAME", # Для модели DeepSeek-R1
      # "--tokenizer", "/models/$MODEL_NAME" # Для модели DeepSeek-R1
      # "--ssl-keyfile", "/secret/privkey.pem",
      # "--ssl-certfile", "/secret/fullchain.pem"
      ]
    profiles:
      - local

  vllm-hf: 
    <<: *default
    volumes:
      - ~/.cache/huggingface/hub":$HF_HOME
    profiles:
      - hf